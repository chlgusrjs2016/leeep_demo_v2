# -*- coding: utf-8 -*-
# ai_service.py - Gemini API í˜¸ì¶œ ê´€ë ¨ í•¨ìˆ˜

import google.generativeai.types as genai_types
import re
import traceback

import config
import prompts

async def generate_response(model, user_message, current_history, current_likability):
    """ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
    print("DEBUG: generate_response - 1ë‹¨ê³„: ì „ì²´ ì‘ë‹µ ìƒì„± ì‹œë„...")
    
    # API ìš”ì²­ì„ ìœ„í•œ íˆìŠ¤í† ë¦¬ ì¤€ë¹„
    history_for_api = current_history.copy()
    history_for_api.append({'role': 'user', 'parts': [user_message]})
    
    # í˜¸ê°ë„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
    likability_percent = f"{current_likability}%"
    likability_context_prompt = f"(ì‹œìŠ¤í…œ ì»¨í…ìŠ¤íŠ¸: ì°¸ê³ ë¡œ í˜„ì¬ ì´ ì‚¬ìš©ìì™€ ë‚˜ì˜ í˜¸ê°ë„ëŠ” {likability_percent} ì•¼. ì´ í˜¸ê°ë„ %ì™€ ë‚˜ì˜ ì—­í•  ì„¤ì •(System Instruction)ì— ëª…ì‹œëœ ê¸°ì¤€ì— ë”°ë¼ ë§íˆ¬ì™€ íƒœë„ë¥¼ ì—„ê²©í•˜ê²Œ ì¡°ì ˆí•´ì„œ ì‘ë‹µí•´ì•¼ í•´. í˜¸ê°ë„ ì ìˆ˜ ìì²´ë¥¼ ì–¸ê¸‰í•˜ì§€ëŠ” ë§ˆ.)"
    history_for_api.append({'role': 'user', 'parts': [likability_context_prompt]})
    
    try:
        # Gemini API í˜¸ì¶œ
        response = await model.generate_content_async(
            history_for_api,
            generation_config=None  # ë˜ëŠ” configì—ì„œ ê°€ì ¸ì˜¨ ì„¤ì •
        )
        
        if not (response and response.text):
            raise Exception("Initial generation failed or blocked")
        
        bot_response_text_full = response.text.strip()
        print(f"DEBUG: generate_response - 1ë‹¨ê³„ ìƒì„± ì „ì²´ ì‘ë‹µ: {bot_response_text_full[:100]}...")
        
        # 2ë‹¨ê³„: ê¸¸ì´ í™•ì¸ ë° í•„ìš”ì‹œ ìš”ì•½ (3ë¬¸ì¥ ì´ˆê³¼ ì‹œ)
        sentences = re.split(r'(?<=[.?!])\s+', bot_response_text_full)
        if len(sentences) > 3:
            print(f"DEBUG: ë‹µë³€ì´ {len(sentences)} ë¬¸ì¥ìœ¼ë¡œ ê¸¸ì–´ì„œ ìš”ì•½ ì‹œë„...")
            summarized_text = await summarize_text(model, bot_response_text_full)
            if summarized_text:
                final_text_to_send = summarized_text
            else:
                print("ê²½ê³ : ìš”ì•½ ì‹¤íŒ¨. ì›ë³¸ ë‹µë³€ì˜ ì²« 3ë¬¸ì¥ ì‚¬ìš©.")
                final_text_to_send = " ".join(sentences[:3])
        else:
            print("DEBUG: ë‹µë³€ì´ 3ë¬¸ì¥ ì´í•˜ì´ë¯€ë¡œ ì›ë³¸ ì‚¬ìš©.")
            final_text_to_send = bot_response_text_full
        
        return bot_response_text_full, final_text_to_send
        
    except Exception as e:
        print(f"ì˜¤ë¥˜: ì‘ë‹µ ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ - {e}")
        traceback.print_exc()
        return "ë¯¸ì•ˆ, ì§€ê¸ˆì€ ë§ì„ ì˜ ëª»í•˜ê² ì–´... ğŸ˜¥", "ë¯¸ì•ˆ, ì§€ê¸ˆì€ ë§ì„ ì˜ ëª»í•˜ê² ì–´... ğŸ˜¥"

async def calculate_likability(model, current_score, message_content):
    """ë©”ì‹œì§€ ê°ì • ë¶„ì„ì„ í†µí•œ í˜¸ê°ë„ ê³„ì‚°"""
    print(f"DEBUG: calculate_likability í˜¸ì¶œë¨ - í˜„ì¬ ì ìˆ˜: {current_score}, ë©”ì‹œì§€: '{message_content[:20]}...'")
    new_score = current_score
    
    try:
        generation_config_sentiment = genai_types.GenerationConfig(**config.SENTIMENT_GENERATION_CONFIG)
        sentiment_prompt = prompts.SENTIMENT_ANALYSIS_PROMPT_TEMPLATE.format(user_message=message_content)
        print(f"DEBUG: ê°ì„± ë¶„ì„ í”„ë¡¬í”„íŠ¸ ì „ì†¡ ì‹œë„")
        
        sentiment_response = await model.generate_content_async(
            sentiment_prompt, 
            generation_config=generation_config_sentiment
        )
        
        if sentiment_response and sentiment_response.text:
            sentiment = sentiment_response.text.strip().upper()
            print(f"DEBUG: ê°ì„± ë¶„ì„ ê²°ê³¼: {sentiment}")
            
            if sentiment == "POSITIVE":
                new_score += config.LIKABILITY_INCREASE_POSITIVE
                print(f"DEBUG: í˜¸ê°ë„ ì¦ê°€! (+{config.LIKABILITY_INCREASE_POSITIVE})")
            elif sentiment == "NEGATIVE":
                new_score -= config.LIKABILITY_DECREASE_NEGATIVE
                print(f"DEBUG: í˜¸ê°ë„ ê°ì†Œ! (-{config.LIKABILITY_DECREASE_NEGATIVE})")
            else:
                print(f"DEBUG: í˜¸ê°ë„ ë³€ê²½ ì—†ìŒ (ê°ì„±: {sentiment})")
        else:
            print(f"ê²½ê³ : ê°ì„± ë¶„ì„ API ì‘ë‹µ ë¹„ì—ˆìŒ. í˜¸ê°ë„ ë³€ê²½ ì—†ìŒ.")
    except Exception as e:
        print(f"ì˜¤ë¥˜: ê°ì„± ë¶„ì„ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    
    # í˜¸ê°ë„ ë²”ìœ„ ì œí•œ
    new_score = max(config.MIN_LIKABILITY_SCORE, min(config.MAX_LIKABILITY_SCORE, new_score))
    print(f"DEBUG: calculate_likability ìµœì¢… ê²°ê³¼ - ìƒˆ ì ìˆ˜: {new_score}")
    
    return new_score

async def summarize_text(model, text_to_summarize):
    """ê¸´ í…ìŠ¤íŠ¸ ìš”ì•½"""
    print(f"DEBUG: summarize_text í˜¸ì¶œë¨ - ìš”ì•½ ëŒ€ìƒ (ì‹œì‘): '{text_to_summarize[:50]}...'")
    
    try:
        summary_prompt = prompts.SUMMARIZE_PROMPT_TEMPLATE.format(text_to_summarize=text_to_summarize)
        generation_config_summary = genai_types.GenerationConfig(**config.SUMMARY_GENERATION_CONFIG)
        
        summary_response = await model.generate_content_async(
            summary_prompt, 
            generation_config=generation_config_summary
        )
        
        if summary_response and summary_response.text:
            summarized_text = summary_response.text.strip()
            print(f"DEBUG: ìš”ì•½ ì„±ê³µ - ìš”ì•½ ê²°ê³¼: {summarized_text}")
            return summarized_text
        else:
            print(f"ê²½ê³ : ìš”ì•½ API ì‘ë‹µ ë¹„ì—ˆê±°ë‚˜ ë¬¸ì œ ìˆìŒ.")
            return None
    except Exception as e:
        print(f"ì˜¤ë¥˜: ìš”ì•½ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
        return None

async def generate_proactive_message(model, user_id, user_display_name):
    """ì„ í†¡ ë©”ì‹œì§€ ìƒì„±"""
    # ì‚¬ìš©ì ë°ì´í„° ë¡œë“œ
    current_history, current_likability = load_user_data(user_id)
    print(f"DEBUG: ì„ í†¡ - ë¡œë“œëœ ê¸°ë¡ (ì´ {len(current_history)} í„´), í˜¸ê°ë„: {current_likability}")
    
    # í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    proactive_instruction = prompts.PROACTIVE_DM_PROMPT_TEMPLATE.format(user_display_name=user_display_name)
    history_with_instruction = current_history.copy()
    history_with_instruction.append({'role': 'user', 'parts': [proactive_instruction]})
    
    try:
        # Gemini API í˜¸ì¶œ
        generation_config = genai_types.GenerationConfig(**config.DEFAULT_GENERATION_CONFIG) if config.DEFAULT_GENERATION_CONFIG else None
        response = await model.generate_content_async(
            history_with_instruction, 
            generation_config=generation_config
        )
        
        if response and response.text:
            generated_text = response.text.strip()
            print(f"Gemini ìƒì„± ë©”ì‹œì§€ (ì„ í†¡, ì „ì²´): {generated_text[:100]}...")
            
            # ìš”ì•½ í•„ìš” ì—¬ë¶€ í™•ì¸
            sentences = re.split(r'(?<=[.?!])\s+', generated_text)
            if len(sentences) > config.SUMMARY_MAX_SENTENCES:
                print(f"DEBUG: ì„ í†¡ - ë‹µë³€ì´ {len(sentences)} ë¬¸ì¥ìœ¼ë¡œ ê¸¸ì–´ì„œ ìš”ì•½ ì‹œë„...")
                summarized_text = await summarize_text(model, generated_text)
                if summarized_text:
                    final_text = summarized_text
                else:
                    print(f"ê²½ê³ : ì„ í†¡ ìš”ì•½ ì‹¤íŒ¨. ì›ë³¸ ì²« {config.SUMMARY_MAX_SENTENCES} ë¬¸ì¥ ì‚¬ìš©.")
                    final_text = " ".join(sentences[:config.SUMMARY_MAX_SENTENCES])
            else:
                final_text = generated_text
            
            return generated_text, final_text  # ì›ë³¸ í…ìŠ¤íŠ¸ì™€ ìµœì¢… í…ìŠ¤íŠ¸ ë°˜í™˜
        else:
            print(f"ê²½ê³ : Gemini ì‘ë‹µ ë¹„ì—ˆê±°ë‚˜ ì°¨ë‹¨ë¨. ê¸°ë³¸ ë©”ì‹œì§€ ì‚¬ìš©.")
            return None, None
    except Exception as e:
        print(f"ì˜¤ë¥˜: Gemini ë©”ì‹œì§€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
        traceback.print_exc()
        return None, None

# database.py ì„í¬íŠ¸ëŠ” í•¨ìˆ˜ ë‚´ë¶€ì—ì„œë§Œ ì‚¬ìš©í•˜ì—¬ ìˆœí™˜ ì°¸ì¡° ë°©ì§€
from database import load_user_data, save_user_data